---
title: DT算法简介
mathjax: true
type: categories
categories: 算法
 
---

#### 简介

​     iDT算法是视频行为理解领域的经典算法，虽然如今深度学习方法在性能上已经超越了iDT，但是深度学习方法与iDT结果融合任然能够带来性能提升[[1]](http://openaccess.thecvf.com/content_iccv_2017/html/Qiu_Learning_Spatio-Temporal_Representation_ICCV_2017_paper.html)。本文先介绍DT算法，iDT算法是DT算法新能的改进版。基本结构与DT[[2]](http://lear.inrialpes.fr/people/wang/download/Dense_trajectories_and_motion_boundary_descriptors_for_action_recognition_IJCV2013.pdf)算法一致。

参考博客：[link](http://blog.csdn.net/wzmsltw/article/details/53023363)

#### 密集轨迹算法(DT算法)

如下图所示，DT算法主要包含**密集特征采样、轨迹跟踪、特征提取**三个主要部分。

![](http://ofltv9hb9.bkt.clouddn.com/20161103_DT_1.jpg)

##### 密集采样

DT方法在不同的空间尺度上进行采样特征点，论文中采用了8个尺度，每个尺度之间相差$1/ \sqrt{2}$。 特征点采样的间隔$W$ 文中取值为5。

DT算法下一步就是在时间序列上跟踪这些采样点，为了更加稳定的对特征点进行跟踪，需要将一些处于纹理稀疏地方的特征点去除，作者使用计算auto-correlation 矩阵的方式进行处理。个人根据代码理解这里的auto-correlation 矩阵为：角点检测中的矩阵$M$。

对于某个尺度的特征点$w(x,y)$ 在领域内计算矩阵$M$：

$M=\sum_{x,y}^{}w(x,y)\begin{bmatrix} {I_x}^2 &{I_xI_y}\\\ {I_xI_y}&{I_y}^2\end{bmatrix}=R^{-1}\begin{bmatrix} \lambda_1 &0\\\ 0&\lambda_2\end{bmatrix}R$

关于角点的详细请见博客：[link](http://blog.csdn.net/xiaowei_cqu/article/details/7805206)

对于计算得到的矩阵$M$ 的特征值，通过阈值机制决定该特征点的去留，具体阈值设置如下：

$T=0.001\times \ \max_{i\in\ I}\min({\lambda_i}^1,{\lambda_i}^2)$   式中$({\lambda_i}^1,{\lambda_i}^2)$指特征点i出的特征值

#### 轨迹描述

描述轨迹需要用到密集光流场。对于密集采样筛选后的某个特征点$P_t=(x_t,y_t)$， 可由下式得到该特征点在下一帧图像中的位置：

$P_{t+1}=(x_{t+1},y_{t+1})=(x_t,y_t)+(M\ast w_t)|_{x_t,y_t}$

上式中，$w_t=(u_t,v_t)$ 是密集光流场，由前后两帧图像($I_t,I_{t+1}$)计算得到，$u$ 与$v$ 分别表示水平与垂直分量，$M$ 为中值滤波器，kernel大小为$3\times 3$。 

某个特征点在连续的$L$ 帧图像间的位置可以构成一段轨迹$(P_t,P_{t+1},\cdots,P_{t+L})$ ，特征提取沿着这段轨迹展开。由于长时间跟踪特征点是不可靠的，所以每隔$L$ 帧重新采样一次特征点，重新跟踪。

轨迹形状特征描述可以用相邻帧间的位移矢量$\Delta P_t=(P_{t+1}-P_t)=(x_{t+1}-x_t,y_{t+1}-y_t)$表示，形状特征可以用向量$(\Delta P_t,\cdots ,\Delta P_{t+L-1})$来描述。最后通过正则化得到轨迹特征描述子。正则化如下：

$T=\frac{(\Delta P_t,\cdots ,\Delta P_{t+L-1})}{\sum_{j=t}^{t+L-1}\| \Delta P_j\|}$

#### HOF,HOG,MBH

文中作者使用了额外三种特征来描述光流，分别是HOF、HOG、MBH。

对于某个特征点长度为$L$的轨迹，在每帧图像上提取特征点周围$N\times N$ 的领域，则能够成一个时空体(volume)，如算法结构图右半部分所示，对于这个**volume** 继续进行更进一步的网格划分，在空间上$x$ 与$y$ 方向上分为$n_\delta$份，时间维度上均匀分为$n_\tau$份。所以在一个**volume**上均匀分为$n_\delta \times n_\delta \times n_\tau$份进行特征提取。在论文中参数取值分别为：$N=32,n_\delta =2,n_\tau =3$ 。

**HOG特征** ：HOG计算每隔小份的灰度图像梯度直方图，直方图bin取值为8，HOG特征长度为96 ($2\times 2\times 3\times 8$)

**HOF特征** ：HOF计算光流幅度信息直方图，直方图bin取9，前8个bin与HOG想吐，最后一个bin用于统计幅值小于某个门限的像素个数。 特征长度为108  ($2\times 2\times 3\times 9$)

**MBH特征** ：MBH 计算光流梯度直方图，由于光流具有$x$ 与$y$ 两个方向，故分别计算MBHx与MBHy，总特征长度为192  ($96\times 2$)

对于计算后的特征，均需要进行$L_2$ 范数归一化。

#### 后续处理

得到特征后，通过特征编码方式得到定长的特征来进行后续分类，文中特征编码用的[bag of features](http://blog.csdn.net/chlele0105/article/details/9633397)方法，分类使用SVM。

##### reference

[1]. Qiu Z, Yao T, Mei T. Learning spatio-temporal representation with pseudo-3d residual networks[C]//2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017: 5534-5542.

[2]. Wang H, Kläser A, Schmid C, et al. Dense trajectories and motion boundary descriptors for action recognition[J]. International journal of computer vision, 2013, 103(1): 60-79.